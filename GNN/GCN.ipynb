{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph CNN with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下这两个函数的作用是使用python3的语法，因为文章可能当初是用python2写的\n",
    "#我把所有内容已经修改成python3的语法了\n",
    "#from __future__ import division\n",
    "#from __future__ import print_function\n",
    "\n",
    "import time  #时间记录模块，python自带的\n",
    "import numpy as np #numpy用于数据读取与存储\n",
    "import scipy.sparse as sp #用于稀疏矩阵的压缩和转化，在数据中可能存在0的数量大于有值的数量的时候才需要使用\n",
    "#可以大幅度提升效率，降低复杂度，无论是存储还是运算\n",
    "import math  #数学工具，提供后文的一些数学计算\n",
    "\n",
    "import torch  #标准torch环境模块，提供tensor类，将numpy数组转化成tensor，让计算机运行效率提升。类似tensorflow\n",
    "import torch.nn as nn #nn是neural network的简称，用以简化搭建神经网络的难度，\n",
    "#你在使用CNN和RNN这些经典层的时候可以直接调用nn.Conv2d,类似keras的使用方式\n",
    "import torch.nn.functional as F  # 提供nn中的一些函数， 例如 F.relu，单列出来是因为functional敲起来太长了\n",
    "import torch.optim as optim # 优化器模块，其实就是中间会调用到两次，固定模板，完全不需要理解，和keras一样\n",
    "\n",
    "\n",
    "#因为pytorch的更新，下面两个基本上是不需要用了，因为variable和tensor整合了\n",
    "#之前tensor是不能算梯度的，所以需要转化成variable，后来pytorch的人嫌麻烦，就改了源代码\n",
    "from torch.nn.parameter import Parameter # 构建的网络的参数，此函数在新版pytorch中已经弃用\n",
    "#新版直接A.parameters就完事了\n",
    "from torch.nn.modules.module import Module # 自己构建的网络需要继承的模块，此函数在新版pytorch中已经弃用\n",
    "#新版直接nn.Module放在类里就可以直接使用了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一段我的个人理解是：\n",
    "\n",
    "load-data——获取数据和邻接矩阵\n",
    "encode_onehot——标签独热码化，在load_data中调用。\n",
    "normalize——将输入归一化，在load_data中调用，类似于batchnormalzation在输入层使用\n",
    "sparse_mx_to_torch_sparse_tensor——用于将稀疏矩阵还原，在load_data中调用\n",
    "\n",
    "load_data是用来将原始的数据按两部分做处理：1、构建邻接矩阵，2、将数据本身的标签和数据转化成两个矩阵\n",
    "返回值是邻接矩阵， 特征（数据）， 标签，\n",
    "encode_onehot是一个函数，用于将原本的标签转化成独热码。如果已经是独热码则无需这一步调用。\n",
    "normalize是一个函数，输入任意矩阵，将矩阵每一行的总和计算出来，加和取倒数，如是稀疏矩阵，将其中无穷值转0。然后取倒数乘积运算。\n",
    "sparse_mx_to_torch_sparse_tensor是给入稀疏矩阵，\n",
    "\n",
    "\n",
    "todense是显示矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, dataset): #此处的path是一个‘//’的路径，dataset应为一个txt、pickle、mat等类型的矩阵\n",
    "    #需要根据输入类型修改下面的数据获取函数\n",
    "    \"\"\"Load citation network dataset \n",
    "       读取数据，该数据应带有链接属性\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Loading {} dataset...'.format(dataset)) #打印一下表明数据来源\n",
    "\n",
    "    # —————————————————————————————————————————————————— #  \n",
    "\n",
    "    # 读取样本id，特征和标签， 此处的np.genfromtxt是一个np内置的函数，用于从txt等列表、元组类型数据存储格式中获取数据\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}{}.content\".format(path, dataset), dtype=np.dtype(\n",
    "            str))  # np.genfromtxt()生成 array： 文件数据的格式为id features    \n",
    "    \n",
    "    #此处原文获取的是文字数据，如果是使用时间序列则需改成如下:\n",
    "    \n",
    "    #from scipy.io import loadmat #通过scipy来读取，这一点和keras是一样的\n",
    "    #matlab_data = loadmat(\"path/dataset.mat\") \n",
    "    \n",
    "    #获取到的数据的格式是一个带key的多元数组\n",
    "    #可以用matlab_data.keys()来返回具体内容（通常是你自己命名的）\n",
    "    \n",
    "    # —————————————————————————————————————————————————— #\n",
    "     #这一步的操作是为了提高内存的运行效率，因为过于稀疏的矩阵的特征转化成密集矩阵，在某种角度其实等同于\n",
    "     #做了一个scale的缩减。\n",
    "    #！但是由于你的输入不一定是特征，也不一定是稀疏矩阵，所以说未必需要使用。\n",
    "    \n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1],\n",
    "                             dtype=np.float32)  # 提取样本的特征，并将其转换为csr矩阵\n",
    "   \n",
    "    #将label转化成独热码\n",
    "    labels = encode_onehot(\n",
    "        idx_features_labels[:, -1])  # 提取样本的标签，并将其转换为one-hot编码形式\n",
    "    \n",
    "    \n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)  # 样本的id数组\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}  # 创建一个字典储存数据id\n",
    "\n",
    "    ### 读取样本之间关系 ： 连边\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(\n",
    "                         edges_unordered.shape)  # 无序边  map 成为有序\n",
    "\n",
    "    # 构建邻接矩阵\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)  # 构建图的邻接矩阵\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(\n",
    "        adj.T >\n",
    "        adj)  # 矩阵进行对称化： 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵。\n",
    "\n",
    "    features = normalize(features)  # 对特征进行归一化处理\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))  #对邻接矩阵进行归一化处理\n",
    "\n",
    "    idx_train = range(140)  # 训练集样本\n",
    "    idx_val = range(200, 500)  # 验证集样本\n",
    "    idx_test = range(500, 1500)  # 测试集样本\n",
    "\n",
    "    # 从 numpy 转换为 torch\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other support function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels): #就是生成label的独热码，你自己做出来的话这一个是可以省略的\n",
    "    classes = set(labels)  # 注意因为 set ，所以每次生成的 y 的 onehot 值是不一样的\n",
    "    classes_dict = {\n",
    "        c: np.identity(len(classes))[i, :]\n",
    "        for i, c in enumerate(classes)\n",
    "    }  # np.identity() 创建一个单位对角阵， 单位矩阵的每一行对应一个one-hot向量\n",
    "    labels_onehot = np.array(\n",
    "        list(map(classes_dict.get, labels)),\n",
    "        dtype=np.int32)  # map(function, iterable)： 对每个 label，应用 class_dict()\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx): #归一化就是压缩到一定范围内，这里是为了降低scale\n",
    "    \"\"\"按行对矩阵进行归一化\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))  # 每行的值进行加和 ：x_sum =  (x_11 + x_12 + x_13 ...)\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # 加和的值取倒数  1/x_sum\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # 将结果中的无穷值转换为 0 ( x_sum可能为0 ，产生无穷值)\n",
    "    r_mat_inv = sp.diags(r_inv)  # 将  1/x_sum 进行对角化\n",
    "    mx = r_mat_inv.dot(mx)  # 初始矩阵和 1/x_sum 对角化矩阵进行乘积运算\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx): #这玩意对你没啥用，是一大堆0的数据才需要用的\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(\n",
    "            np.int64))  # # 获得稀疏矩阵坐标 (2708, 1433)  --> (49216, 2)\n",
    "    values = torch.from_numpy(sparse_mx.data)  # 相应位置的值 (49216, ) 即矩阵中的所有非零值\n",
    "    shape = torch.Size(sparse_mx.shape)  # 稀疏矩阵的大小\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph CNN Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN有两种实现模式，一种是：设置GCN类，如下；另一种是在forward中加入点积即可\n",
    "这里把GCN描述一遍，后面在forward那里我会给个加入点积位置的注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    #一个神经网络层、或者其他功能层的类在设置时通常涉及如下部分：\n",
    "    #初始化：用于确认输入输出和一些基本的自带的属性，例如权重，偏置（通常偏置不需要你考虑，默认有就行了)\n",
    "    #参数更新模式：用于更新内部的属性的输入接口，否则属性将无法自动更新，因为标准模块中名字是一样的，直接继承过来可以省去很多功夫\n",
    "    #参数初始化模式：用于初始化parameter，因为parameter类中的初始化通常是0初始化\n",
    "    #层内前向通路：forward，核心关键。在卷积层中这一部分是依次卷积，一重循环，在图卷积中则使用图卷积的前向过程\n",
    "                   #其实就是变量怎么变换，想清楚了可以自定义任何计算层。\n",
    "    #覆盖返回名称：通常init里面定义的内容，从外部直接用点就可以返回，如果想要好看一点，就用这个部分来设置。\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()  # 确保父类被正确的初始化了 #这里是为了能够继承标准层中的paramet这一个类，以简化后面的设置\n",
    "\n",
    "        self.in_features = in_features #输入维度\n",
    "        self.out_features = out_features #输出维度\n",
    "        self.weight = Parameter(\n",
    "            torch.FloatTensor(in_features, out_features)\n",
    "        )  \n",
    "        # 当Paramenters赋值给Module的属性的时候，他会自动的被加到 Module的 参数列表中 \n",
    "        # 继承标准的parameter类，就可以更新了\n",
    "        # 此处的torch.FloatTensor是一个将后面内容视作维度创建矩阵张量的函数。 parameter这个实现参数化\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))  #这里可以看到如果输入一个维度，则生成一个1×N的矩阵可以用以加偏置\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters() # 进行参数初始化\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"参数初始化方式\"\"\"\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv) # 权重满足 #这里是使用uniform也就是高斯分布直接初始化\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv) #同理初始化偏置\n",
    "\n",
    "    def forward(self, input_matrix, adj):\n",
    "        #应该注意的是，在创建GNN的时候，使用\n",
    "        \n",
    "        support = torch.matmul(input_matrix, self.weight) # 将输入特征矩阵与权重参数矩阵相点积，注意这里有一个adj代表邻接矩阵\n",
    "        #forward中需要给adj，因此在使用这一类的时候，会从类外环境调用adj这一个参数\n",
    "        output = torch.spmm(adj, support) # 左乘标准化的邻接矩阵，邻接矩阵的存储时用的是稀疏矩阵 \n",
    "        #这里如果之前不用稀疏矩阵这些东西，应该是改成:\n",
    "        #output = torch.matmul(adj, support)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"输出类内部变量的名称\"\"\"\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GCN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原本那个GCN的长度太短，这里这个是个长的。加入了一些\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, model, num_classes, in_channel, t=0, adj_file=None):\n",
    "        super(GCNResnet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "        ) #这个类似于keras的sequential创建，里面的内容可以更改.但是有一说一写习惯之后会觉得不是很好用\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(3, 2) #池化层使用模板\n",
    "        self.dropout = dropout #可以定义dropout\n",
    "        \n",
    "        #正常使用的方式是：\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=(1, 3),stride=1,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=16*7*7,out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "\n",
    "        self.gc1 = GraphConvolution(in_channel, 1024) #设置图卷积的init内容\n",
    "        self.gc2 = GraphConvolution(1024, 2048)\n",
    "        self.relu = nn.LeakyReLU(0.2) #这里你可以不写，直接在forward里面a = F.relu(a)就可以实现\n",
    "        #这里是给你看看其他激活函数咋整\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        #第一种写法就是使用自己新建的类\n",
    "        x = F.relu(self.features(x, adj)) # 第一层图卷积 \n",
    "        x = F.dropout(x, self.dropout, training=self.training) \n",
    "        x = self.gc2(x, adj) # 第二层图卷积\n",
    "        \n",
    "        #如果是想要实现整体预测分类，你只需要保证最后的一层的输出节点数等于分类的类别数\n",
    "        #然后最后用argmax+softmax来取值就好\n",
    "        \n",
    "        #第二种写法是直接用CNN，后面跟个mm：\n",
    "        #x = torch.matmul(adj, x)\n",
    "        #x = F.relu(self.conv1(input_size, output_size))\n",
    "        #x = F.dropout(x, self.dropout, training=self.training) \n",
    "        #大概就类似这种，在卷积前×一个就好了\n",
    "        \n",
    "        #但是有一个问题，就是无论使用哪种图卷积的操作，应该保证维度要是对的，因为你后面跟个卷积，维度可能发生改变。\n",
    "        #因为mm这类矩阵乘法需要用m*n与n*q的矩阵相乘得到结果，如果你得到的x的n*q发生了改变，adj和x的计算就会出问题。\n",
    "        #我建议你看看别人这个地方是怎么处理的。\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) # 计算 softmax + log 输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch的运行非常简单，分为以下部分：\n",
    "1、超参数设置\n",
    "2、实例化\n",
    "3、优化器实例化\n",
    "4、训练\n",
    "5、测试画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例化的超参数设置主要包含以下部分：\n",
    "seed用来确认随机数，使得结果可复现：相当于是伪随机。\n",
    "epochs迭代次数。\n",
    "lr学习率。\n",
    "weight_decay是一个正则化项。\n",
    "hidden是隐层数量，可以在建模型的时候手动指定，也可以外部指定：外部指定的作用是可以做一个循环：for hidden in range 1，100.这样就可以尝试多种hidden的结果。\n",
    "dropout你懂的。\n",
    "fastmode = False 这一个选项是：dropout和BN在训练和测试的时候其实并不一样:测试的时候dropout是不运作的。默认false就好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "epochs =200 # Number of epochs to train.\n",
    "lr = 0.01 # Initial learning rate.\n",
    "weight_decay = 5e-4 # Weight decay (L2 loss on parameters)\n",
    "hidden = 16 # Number of hidden units.'\n",
    "dropout = 0.5 # Dropout rate (1 - keep probability)\n",
    "fastmode = False # val 时候是否和训练区分（dropout， BN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据与模型实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'model', 'num_classes', and 'in_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-aaf6b066e3e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 构建模型 #主要是设置内部的参数，其实如果你一开始就确定了参数，这里直接：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mGCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#就可以了\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#model = GCN(nfeat=features.shape[1],nhid=hidden,nclass=labels.max().item() + 1,dropout=dropout)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'model', 'num_classes', and 'in_channel'"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# 构建模型 #主要是设置内部的参数，其实如果你一开始就确定了参数，这里直接：\n",
    "# model =GCN()就可以了\n",
    "model = GCN(nfeat=features.shape[1],nhid=hidden,nclass=labels.max().item() + 1,dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器与损失函数实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-18f622696c23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 创建优化器-因为pytorch当中优化器和反向传播全是pytorch来完成，\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 所以说需要在实例化的时候把这个设置好，后面一行代码解决\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 包含了权重正则化部分的 loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 创建优化器-因为pytorch当中优化器和反向传播全是pytorch来完成，\n",
    "# 所以说需要在实例化的时候把这个设置好，后面一行代码解决\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # 包含了权重正则化部分的 loss\n",
    "\n",
    "#损失函数实例化也可以放到训练里面，其实无所谓在哪，能用就行\n",
    "loss_train = F.nll_loss(\n",
    "        output[idx_train], labels[idx_train]\n",
    "    )  # ；计算损失与准确率；交叉熵loss， 因为模型计算包含 log， 原文这里使用 nll_loss（CrossEntropyLoss =Softmax+Log+NLLLoss）\n",
    "#实际上如果你想用交叉熵：\n",
    "loss_func = nn.CrossEntropyLoss() \n",
    "#然后再在训练中loss_func(y, label)就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中的网络模型训练非常非常模块化：\n",
    "可以总结为：\n",
    "设置自己是训练模式——把梯度置零——y = model（data，adj）——计算损失函数——一句话反向传播——一句话更新梯度\n",
    "\n",
    "然后测试有好几种方式：你可以在训练一定次数后测试，也可以全整完之后测试，只需要把测试的位置改一改就行了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简单的准确率计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    \"\"\"计算准确率\"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)  # 类型转换\n",
    "    correct = preds.eq(labels).double()  # 是否相同， true， false\n",
    "    correct = correct.sum()  # true false 加和\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练主体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch): #这个内部的epoch其实没啥用，不用在意，只是为了迭代次数而已\n",
    "    \"\"\"标准 pytorch 神经网络流程\"\"\"\n",
    "    t = time.time() # 这个是为了读时间，其实也不是必要的\n",
    "    model.train()  # 先将model置为训练状态 #其实如果没有dropout，这一句是可以不要的\n",
    "    \n",
    "    output = model(features, adj) # 将输入送到模型得到输出结果 y = GCN(data, adj)\n",
    "    loss_func = F.nll_loss(\n",
    "        output[idx_train], labels[idx_train]\n",
    "    )  #其实就是类似loss = CrossEntropyLoss(y, label)\n",
    "    # ；计算损失与准确率；交叉熵loss， 因为模型计算包含 log， 这里使用 nll_loss（CrossEntropyLoss =Softmax+Log+NLLLoss）\n",
    "    #如果前面定义了loss这里直接loss_func(y, label)就行\n",
    "    \n",
    "    #计算准确率\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！#\n",
    "    #这三句就是任何神经网络中的梯度更新，任何模型照搬就行\n",
    "    optimizer.zero_grad() # 梯度置0\n",
    "    loss_func.backward() # 反向传播求梯度 loss_func是自己命名的名字\n",
    "    optimizer.step() # 更新参数\n",
    "    #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！#\n",
    "    \n",
    "    #用来进行测试\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval(\n",
    "        )  #pytorch会自动把BN和DropOut固定住, dropout和batch normalization的操作在训练和测试的时候是不一样的\n",
    "        #如果没有用dropout，就不用管，直接算就行\n",
    "        output = model(features, adj)#y = GCN(data, adj)\n",
    "\n",
    "    #类似上面的，这里的accuracy函数其实就是一个简单的计算函数\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    #打印\n",
    "    print('Epoch: {:04d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "#这一步其实就是之前测试的重复，只是最后再算一次而已\n",
    "def test():\n",
    "    model.eval() # 置为 evaluation 状态 \n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\", \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例化训练开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
